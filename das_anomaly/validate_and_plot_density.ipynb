{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the trained model and choose a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from das_anomaly import calculate_percentile, density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters and the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path to result model and plots\n",
    "results_path = \"/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/background_noise/results/\"\n",
    "\n",
    "# Size of the input images\n",
    "size = 128\n",
    "\n",
    "# Define generators for training, validation and also anomaly data.\n",
    "batch_size = 64\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# path to training PSD plots (seen data)\n",
    "train_path = \"/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/background_noise/plots/train/\"\n",
    "num_train_data = 768\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path, target_size=(size, size), batch_size=batch_size, class_mode=\"input\"\n",
    ")\n",
    "\n",
    "# path to testing PSD plots (unseen data)\n",
    "test_path = \"/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/background_noise/plots/test/\"\n",
    "num_test_data = 192\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    test_path, target_size=(size, size), batch_size=batch_size, class_mode=\"input\"\n",
    ")\n",
    "\n",
    "# path to known anomalies or events\n",
    "events_path = \"/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/seismic_events/plots/obvious_seismic_events/\"\n",
    "anomaly_generator = datagen.flow_from_directory(\n",
    "    events_path, target_size=(size, size), batch_size=batch_size, class_mode=\"input\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load load the saved trained model and its history\n",
    "model_path = os.path.join(results_path, \"model_1_128\")\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "with open(os.path.join(results_path, \"history_1_128.json\")) as json_file:\n",
    "    history_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check anomolous and normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "data_batch_validation = []\n",
    "batch_num = 0\n",
    "while batch_num <= validation_generator.batch_index:  # Gets each generated batch of size batch_size\n",
    "    data = next(validation_generator)\n",
    "    data_batch_validation.append(data[0])\n",
    "    batch_num = +1\n",
    "\n",
    "predicted = loaded_model.predict(data_batch_validation[0])  # Predict on the first batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check normal data and view an image and corresponding reconstruction\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch_validation[0][image_number])\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.title(\"predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the reconstruction error between our validation data (normal images)\n",
    "# and the anomaly images\n",
    "validation_error = loaded_model.evaluate(validation_generator)\n",
    "\n",
    "# Evaluate the model using the anomaly data generator\n",
    "anomaly_error = loaded_model.evaluate(anomaly_generator)\n",
    "\n",
    "print(\"Reconstruction error for the normal data is: \", validation_error)\n",
    "print(\"Reconstruction error for the anomaly data is: \", anomaly_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "data_batch_anomaly = []\n",
    "img_num = 0\n",
    "while img_num <= anomaly_generator.batch_index:  # Gets each generated batch of size batch_size\n",
    "    data = next(anomaly_generator)\n",
    "    data_batch_anomaly.append(data[0])\n",
    "    img_num = img_num + 1\n",
    "\n",
    "predicted = loaded_model.predict(data_batch_anomaly[0])  # Predict on the first batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check anomalous data: plot an image and corresponding reconstructions\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch_anomaly[0][image_number])\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.title(\"predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract (or build) the encoder network, with trained weights.\n",
    "# This is used to get the compressed output (latent space) of the input image.\n",
    "# The compressed output is then used to calculate the KDE\n",
    "\n",
    "encoder_model = Sequential()\n",
    "# Add the convolutional layer without weights\n",
    "encoder_model.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(size, size, 3)))\n",
    "# Set the weights from the corresponding layer of the loaded model\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[0].get_weights())\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "encoder_model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[2].get_weights())\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "encoder_model.add(Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[4].get_weights())\n",
    "\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate kernel density estimation (KDE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute latent space representations\n",
    "encoded_images = encoder_model.predict(train_generator)\n",
    "\n",
    "# Flatten latent space representations because KDE from sklearn takes 1D vectors as input\n",
    "encoder_output_shape = encoder_model.output_shape\n",
    "out_vector_shape = encoder_output_shape[1] * encoder_output_shape[2] * encoder_output_shape[3]\n",
    "\n",
    "encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in encoded_images]\n",
    "\n",
    "# Fit a kernel density model to the latent representations\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(encoded_images_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all_train_batches and all_anomaly_batches as needed\n",
    "normal_values = density(encoder_model, np.vstack(data_batch_validation), kde)\n",
    "anomolous_values = density(encoder_model, np.vstack(data_batch_anomaly), kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the histogram and choose a thereshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram for density score\n",
    "plt.hist(normal_values, bins=2, alpha=0.5, label=\"normal_values\")\n",
    "plt.hist(anomolous_values, bins=10, alpha=0.5, label=\"anomolous_values\")\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"Density score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a thereshold based on desired percentile\n",
    "percentile = 95\n",
    "percentile_value = calculate_percentile(anomolous_values, percentile)\n",
    "print(f\"The {percentile}th percentile is: {percentile_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_das_anomaly310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
