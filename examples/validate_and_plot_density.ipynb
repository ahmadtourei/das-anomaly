{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the trained model and choose a threshold\n",
    "\n",
    "This jupyter notebook helps user to visualize and validate results form their trained model. Also, it helps usdsr to pick a threshold based on the: 1) density score of the validation dataset (which does not include any images with anomalies), and 2) some known examples of images with anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from das_anomaly import calculate_percentile, density\n",
    "from das_anomaly.settings import SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import parameters and define the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the specified path to the saved trained model from pre-defined config_user.py\n",
    "results_path = SETTINGS.TRAINED_PATH\n",
    "\n",
    "# Import the size of the input images\n",
    "size = SETTINGS.SIZE\n",
    "\n",
    "# Import the define generators for training, validation and also anomaly data.\n",
    "batch_size = SETTINGS.BATCH_SIZE\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Path to training PSD plots (seen data)\n",
    "train_path = SETTINGS.TRAIN_IMAGES_PATH\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path, target_size=(size, size), batch_size=batch_size, class_mode=\"input\"\n",
    ")\n",
    "\n",
    "# Path to testing PSD plots (unseen data)\n",
    "test_path = SETTINGS.TEST_IMAGES_PATH\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    test_path, target_size=(size, size), batch_size=batch_size, class_mode=\"input\"\n",
    ")\n",
    "\n",
    "# Path to known anomalies\n",
    "events_path = SETTINGS.ANOMALY_IMAGES_PATH\n",
    "anomaly_generator = datagen.flow_from_directory(\n",
    "    events_path, target_size=(size, size), batch_size=batch_size, class_mode=\"input\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved trained model and its history\n",
    "model_path = os.path.join(results_path, f\"model_{size}.h5\")\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Compile the model with the same trained parameters\n",
    "loaded_model.compile(\n",
    "    optimizer=\"adam\", loss=\"mse\", metrics=[keras.metrics.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "with open(os.path.join(results_path, f\"history_{size}.json\")) as json_file:\n",
    "    history_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check anomolous and normal, event-free data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-free data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "data_batch_validation = []\n",
    "batch_num = 0\n",
    "while (\n",
    "    batch_num <= validation_generator.batch_index\n",
    "):  # Gets each generated batch of size batch_size\n",
    "    data = next(validation_generator)\n",
    "    data_batch_validation.append(data[0])\n",
    "    batch_num = +1\n",
    "\n",
    "predicted = loaded_model.predict(\n",
    "    data_batch_validation[0]\n",
    ")  # Predict on the first batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check normal (event-free) data and view an image and corresponding reconstruction\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch_validation[0][image_number])\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.title(\"predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "data_batch_anomaly = []\n",
    "img_num = 0\n",
    "while (\n",
    "    img_num <= anomaly_generator.batch_index\n",
    "):  # Gets each generated batch of size batch_size\n",
    "    data = next(anomaly_generator)\n",
    "    data_batch_anomaly.append(data[0])\n",
    "    img_num = img_num + 1\n",
    "\n",
    "predicted = loaded_model.predict(\n",
    "    data_batch_anomaly[0]\n",
    ")  # Predict on the first batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check anomalous data: plot an image and corresponding reconstructions\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch_anomaly[0][image_number])\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.title(\"predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the MSE reconstruction error between event-free and anomalous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the reconstruction error between our validation data (which are normal, event-free images) and the anomalous images\n",
    "validation_error = loaded_model.evaluate(validation_generator)\n",
    "\n",
    "# Evaluate the model using the anomaly data generator\n",
    "anomaly_error = loaded_model.evaluate(anomaly_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder network, with trained weights.\n",
    "# This is used to get the compressed output (latent space) of the input image.\n",
    "# The compressed output is then used to calculate the KDE and density scores\n",
    "encoder_model = Sequential()\n",
    "# Add the convolutional layer without weights\n",
    "encoder_model.add(\n",
    "    Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(size, size, 3))\n",
    ")\n",
    "# Set the weights from the corresponding layer of the loaded model\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[0].get_weights())\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "encoder_model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[2].get_weights())\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "encoder_model.add(Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[4].get_weights())\n",
    "\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate kernel density estimation (KDE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute latent space representations\n",
    "encoded_images = encoder_model.predict(train_generator)\n",
    "\n",
    "# Flatten latent space representations because KDE from sklearn takes 1D vectors as input\n",
    "encoder_output_shape = encoder_model.output_shape\n",
    "out_vector_shape = (\n",
    "    encoder_output_shape[1] * encoder_output_shape[2] * encoder_output_shape[3]\n",
    ")\n",
    "\n",
    "encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in encoded_images]\n",
    "\n",
    "# Fit a kernel density model to the latent representations\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.2).fit(encoded_images_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all_train_batches and all_anomaly_batches as needed\n",
    "normal_values = density(encoder_model, np.vstack(data_batch_validation), kde)\n",
    "anomolous_values = density(encoder_model, np.vstack(data_batch_anomaly), kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the histogram and choose a thereshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each observation into “1 / total × 100” so every bar shows % of samples\n",
    "w_normal = np.ones_like(normal_values) / normal_values.size * 100\n",
    "w_anom = np.ones_like(anomolous_values) / anomolous_values.size * 100\n",
    "\n",
    "# Plotting the histogram for density score\n",
    "plt.hist(\n",
    "    normal_values, bins=1, weights=w_normal, alpha=0.5, label=\"data without anomalies\"\n",
    ")\n",
    "plt.hist(\n",
    "    anomolous_values, bins=20, weights=w_anom, alpha=0.5, label=\"data with anomalies\"\n",
    ")\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"Density score\")\n",
    "plt.ylabel(\"Percentage of observations (%)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a thereshold based on desired percentile and then update the config_user module and use it for anomaly detection (in the detect_anomalies scripts)\n",
    "percentile = 95\n",
    "percentile_value = calculate_percentile(anomolous_values, percentile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
