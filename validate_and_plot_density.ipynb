{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the trained model and choose a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "utils_dir = os.path.join(current_dir, 'source')\n",
    "sys.path.append(utils_dir)\n",
    "from utils import calculate_percentile, density, read_data_from_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters and the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path to result model and plots\n",
    "results_path = \"/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/background_noise/results/\"\n",
    "\n",
    "# Size of the input images\n",
    "size = 128\n",
    "\n",
    "# Define generators for training, validation and also anomaly data.\n",
    "batch_size = 64\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# path to training PSD plots (seen data)\n",
    "train_path = '/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/background_noise/plots/train/'\n",
    "num_train_data = 768\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(size, size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n",
    "\n",
    "# path to testing PSD plots (unseen data)\n",
    "test_path = '/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/background_noise/plots/test/'\n",
    "num_test_data = 192\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(size, size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n",
    "\n",
    "# path to known anomalies or events\n",
    "events_path = '/u/pa/nb/tourei/scratch/caserm/spectrum_analysis/seismic_events/plots/obvious_seismic_events/'\n",
    "anomaly_generator = datagen.flow_from_directory(\n",
    "    events_path,\n",
    "    target_size=(size, size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load load the saved trained model and its history\n",
    "model_path = os.path.join(results_path, \"model_1_128\")\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "with open(os.path.join(results_path, 'history_1_128.json'), 'r') as json_file:\n",
    "    history_dict = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check anomolous and normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "data_batch = []  \n",
    "batch_num = 0\n",
    "while batch_num <= validation_generator.batch_index: # Gets each generated batch of size batch_size\n",
    "    data = next(validation_generator)\n",
    "    data_batch.append(data[0])\n",
    "    batch_num =+ 1\n",
    "\n",
    "predicted = loaded_model.predict(data_batch[0]) # Predict on the first batch of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check normal data and view an image and corresponding reconstruction\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch[0][image_number])\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.title(\"predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the reconstruction error between our validation data (normal images)\n",
    "# and the anomaly images\n",
    "validation_error = loaded_model.evaluate(validation_generator)\n",
    "\n",
    "# Evaluate the model using the anomaly data generator\n",
    "anomaly_error = loaded_model.evaluate(anomaly_generator)\n",
    "\n",
    "print(\"Reconstruction error for the normal data is: \", validation_error)\n",
    "print(\"Reconstruction error for the anomaly data is: \", anomaly_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "data_batch = []  \n",
    "img_num = 0\n",
    "while img_num <= anomaly_generator.batch_index: # Gets each generated batch of size batch_size\n",
    "    data = next(anomaly_generator)\n",
    "    data_batch.append(data[0])\n",
    "    img_num = img_num + 1\n",
    "\n",
    "predicted = loaded_model.predict(data_batch[0]) # Predict on the first batch of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check anomalous data: plot an image and corresponding reconstructions\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch[0][image_number])\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.title(\"predicted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract (or build) the encoder network, with trained weights.\n",
    "#This is used to get the compressed output (latent space) of the input image. \n",
    "#The compressed output is then used to calculate the KDE\n",
    "\n",
    "encoder_model = Sequential()\n",
    "# Add the convolutional layer without weights\n",
    "encoder_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(size, size, 3)))\n",
    "# Set the weights from the corresponding layer of the loaded model\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[0].get_weights())\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "encoder_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[2].get_weights())\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "encoder_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "encoder_model.layers[-1].set_weights(loaded_model.layers[4].get_weights())\n",
    "\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate kernel density estimation (KDE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoded output of input images = Latent space\n",
    "encoded_images = encoder_model.predict(train_generator)\n",
    "\n",
    "# Flatten the encoder output because KDE from sklearn takes 1D vectors as input\n",
    "encoder_output_shape = encoder_model.output_shape # Here, we have 16x16x16\n",
    "out_vector_shape = encoder_output_shape[1] * encoder_output_shape[2] * encoder_output_shape[3]\n",
    "\n",
    "encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in encoded_images]\n",
    "\n",
    "# Fit KDE to the image latent data\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(encoded_images_vector)\n",
    "\n",
    "# Get average and std dev. of density and recon. error for normal and anomaly images. \n",
    "# Need to generate a batch of images for each. \n",
    "train_batch = next(train_generator)[0]\n",
    "anomaly_batch = next(anomaly_generator)[0]\n",
    "\n",
    "normal_values_128 = density(encoder_model, train_batch, kde)\n",
    "anomolous_values_128 = density(encoder_model, anomaly_batch, kde)\n",
    "\n",
    "np.savetxt('normal_values_128.txt', normal_values_128, delimiter=',')\n",
    "np.savetxt('anomolous_values_128.txt', anomolous_values_128, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_batches = []\n",
    "batch_num = 0\n",
    "# Loop through the train_generator to collect all batches\n",
    "while batch_num <= train_generator.batch_index: # Gets each generated batch of size batch_size\n",
    "    print(\"batch_num = \", batch_num)\n",
    "    data = next(train_generator)\n",
    "    all_train_batches.append(data[0])\n",
    "    batch_num =+ 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_batches = []\n",
    "batch_num = 0\n",
    "# Loop through the anomaly_generator to collect all batches\n",
    "while batch_num <= anomaly_generator.batch_num: # Gets each generated batch of size batch_size\n",
    "    print(\"batch_num = \", batch_num)\n",
    "    data = next(anomaly_generator)\n",
    "    all_anomaly_batches.append(data[0])\n",
    "    batch_num =+ 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all_train_batches and all_anomaly_batches as needed\n",
    "normal_values = density(encoder_model, all_train_batches, kde)\n",
    "anomolous_values = density(encoder_model, all_anomaly_batches, kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the histogram and choose a thereshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram for density score\n",
    "plt.hist(normal_values, bins=2, alpha=0.5, label='normal_values')\n",
    "plt.hist(anomolous_values, bins=10, alpha=0.5, label='anomolous_values')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Density score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a thereshold based on desired percentile\n",
    "file_path = 'anomaly_values_128.txt' \n",
    "percentile = 95\n",
    "data = read_data_from_file(file_path)\n",
    "percentile_value = calculate_percentile(data, percentile)\n",
    "\n",
    "print(f\"The {percentile}th percentile is: {percentile_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpih5py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
